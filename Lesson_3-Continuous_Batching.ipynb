{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:26:12.213593Z",
     "start_time": "2025-03-11T13:26:12.210874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import helpers\n",
    "# from helpers import init_batch, generate_next_token\n",
    "# from helpers import merge_batches, filter_batch"
   ],
   "id": "72e9f9a8994968ed",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-11T13:26:14.882649Z",
     "start_time": "2025-03-11T13:26:12.216946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:26:45.964512Z",
     "start_time": "2025-03-11T13:26:14.900316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ],
   "id": "a0907464a17bdc73",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:26:45.988503Z",
     "start_time": "2025-03-11T13:26:45.985875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n"
   ],
   "id": "84321372cc844e12",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:26:46.002353Z",
     "start_time": "2025-03-11T13:26:45.997127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# multiple prompts of varying lengths to send to the model at once\n",
    "prompts = [\n",
    "    \"The quick brown fox jumped over the\",\n",
    "    \"The rain in Spain falls\",\n",
    "    \"What comes up must\",\n",
    "]\n",
    "\n",
    "# note: padding=True ensures the padding token\n",
    "# will be inserted into the tokenized tensors\n",
    "inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\")"
   ],
   "id": "95a2403557408717",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:26:46.025302Z",
     "start_time": "2025-03-11T13:26:46.019965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def generate_batch_tokens_with_past(inputs):\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     logits = outputs.logits\n",
    "#     last_logits = logits[:, -1, :]\n",
    "#     next_token_ids = last_logits.argmax(dim=1)\n",
    "#     return next_token_ids, outputs.past_key_values\n",
    "\n",
    "\n",
    "def generate_batch(inputs, max_tokens):\n",
    "    # create a list of tokens for every input in the batch\n",
    "    generated_tokens = [[] for _ in range(inputs[\"input_ids\"].shape[0])]\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "    next_inputs = {\n",
    "        \"position_ids\": position_ids,\n",
    "        **inputs\n",
    "    }\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_ids, past_key_values = generate_batch_tokens_with_past(next_inputs)\n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_ids.reshape(-1, 1),\n",
    "            \"position_ids\": next_inputs[\"position_ids\"][:, -1].unsqueeze(-1) + 1,\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [next_inputs[\"attention_mask\"], torch.ones((next_token_ids.shape[0], 1))],\n",
    "                dim=1\n",
    "            ),\n",
    "            \"past_key_values\": past_key_values\n",
    "        }\n",
    "\n",
    "        next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "        for i, token in enumerate(next_tokens):\n",
    "            generated_tokens[i].append(token)\n",
    "\n",
    "    return [\"\".join(tokens) for tokens in generated_tokens]\n"
   ],
   "id": "ebc9c7d9c28736b1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:26:46.029069Z",
     "start_time": "2025-03-11T13:26:46.027579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# seed the random number generator, so our results are deterministic\n",
    "random.seed(42)\n",
    "\n",
    "# constants\n",
    "queue_size = 32\n",
    "batch_size = 8\n",
    "\n",
    "# requests waiting to be processed\n",
    "# requests are tuples of (prompt, max_tokens)\n",
    "request_queue = [\n",
    "    (prompts[i % len(prompts)], 100 if i % batch_size == 0 else 10)\n",
    "    for i in range(queue_size)\n",
    "]\n",
    "\n"
   ],
   "id": "720395eea0c7599a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:26:46.037941Z",
     "start_time": "2025-03-11T13:26:46.035110Z"
    }
   },
   "cell_type": "code",
   "source": "request_queue[:8]",
   "id": "963944c01d30e48f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The quick brown fox jumped over the', 100),\n",
       " ('The rain in Spain falls', 10),\n",
       " ('What comes up must', 10),\n",
       " ('The quick brown fox jumped over the', 10),\n",
       " ('The rain in Spain falls', 10),\n",
       " ('What comes up must', 10),\n",
       " ('The quick brown fox jumped over the', 10),\n",
       " ('The rain in Spain falls', 10)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:26:46.045453Z",
     "start_time": "2025-03-11T13:26:46.043926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batches = [\n",
    "    request_queue[i: i + batch_size]\n",
    "    for i in range(0, len(request_queue), batch_size)\n",
    "]"
   ],
   "id": "7ea00ccf7055a040",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:26:46.061002Z",
     "start_time": "2025-03-11T13:26:46.059199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# t0 = time.time()\n",
    "# with tqdm(total=len(batches), desc=f\"bs={batch_size}\") as pbar:\n",
    "#     for i, batch in enumerate(batches):\n",
    "#         batch_max_tokens = [bs[1] for bs in batch]\n",
    "#         max_tokens = max(batch_max_tokens)\n",
    "#         pbar.set_postfix({'max_tokens': max_tokens})\n",
    "#\n",
    "#         batch_prompts = [b[0] for b in batch]\n",
    "#         inputs = tokenizer(\n",
    "#             batch_prompts, padding=True, return_tensors=\"pt\")\n",
    "#         generate_batch(inputs, max_tokens=max_tokens)\n",
    "#\n",
    "#         pbar.update(1)\n",
    "#\n",
    "# duration_s = time.time() - t0\n",
    "# print(\"duration: \", duration_s)\n",
    "\n"
   ],
   "id": "51403e676664535c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Let's try continuous batching\n",
   "id": "8addc69597f0f55c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:40:39.519684Z",
     "start_time": "2025-03-11T13:40:39.514380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# seed the random number generator, so out results are deterministic\n",
    "random.seed(42)\n",
    "\n",
    "# constants\n",
    "queue_size = 32\n",
    "batch_size = 8\n",
    "\n",
    "# requests waiting to be processed\n",
    "# this time requests are tuples (prompt, max_tokens)\n",
    "request_queue = [\n",
    "    (prompts[i % len(prompts)], 100 if i % batch_size == 0 else 10)\n",
    "    for i in range(queue_size)\n",
    "]\n",
    "print(request_queue)\n",
    "\n"
   ],
   "id": "acbeabc857fe8277",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The quick brown fox jumped over the', 100), ('The rain in Spain falls', 10), ('What comes up must', 10), ('The quick brown fox jumped over the', 10), ('The rain in Spain falls', 10), ('What comes up must', 10), ('The quick brown fox jumped over the', 10), ('The rain in Spain falls', 10), ('What comes up must', 100), ('The quick brown fox jumped over the', 10), ('The rain in Spain falls', 10), ('What comes up must', 10), ('The quick brown fox jumped over the', 10), ('The rain in Spain falls', 10), ('What comes up must', 10), ('The quick brown fox jumped over the', 10), ('The rain in Spain falls', 100), ('What comes up must', 10), ('The quick brown fox jumped over the', 10), ('The rain in Spain falls', 10), ('What comes up must', 10), ('The quick brown fox jumped over the', 10), ('The rain in Spain falls', 10), ('What comes up must', 10), ('The quick brown fox jumped over the', 100), ('The rain in Spain falls', 10), ('What comes up must', 10), ('The quick brown fox jumped over the', 10), ('The rain in Spain falls', 10), ('What comes up must', 10), ('The quick brown fox jumped over the', 10), ('The rain in Spain falls', 10)]\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:40:39.620441Z",
     "start_time": "2025-03-11T13:40:39.608580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_batch_tokens_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "    return next_token_ids, outputs.past_key_values\n",
    "\n",
    "\n",
    "def init_batch(request_queue):\n",
    "    \"\"\"\n",
    "    加工为模型输入的批量。\n",
    "    进行batch内对齐\n",
    "    :param request_queue:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    prompts, max_tokens = zip(*request_queue)\n",
    "    inputs = tokenizer(list(prompts), padding=True, return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"max_tokens\": torch.tensor(max_tokens).long(),\n",
    "    }\n",
    "\n",
    "\n",
    "def pad_to_max_length(tensor, max_length, pad_value=0):\n",
    "    pad_size = max_length - tensor.size(1)\n",
    "    if pad_size > 0:\n",
    "        padding = torch.full((tensor.size(0), pad_size), pad_value, dtype=tensor.dtype, device=tensor.device)\n",
    "        tensor = torch.cat([tensor, padding], dim=1)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def merge_batches(cached_batch, new_batch):\n",
    "    \"\"\"\n",
    "    合并新的请求到缓存批量。\n",
    "    根据更大的batch进行对齐\n",
    "    :param cached_batch:\n",
    "    :param new_batch:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    max_length = max(cached_batch[\"input_ids\"].size(1), new_batch[\"input_ids\"].size(1))\n",
    "    # Pad input_ids, attention_mask, and position_ids to the max_length\n",
    "    cached_batch[\"input_ids\"] = pad_to_max_length(cached_batch[\"input_ids\"], max_length, pad_value=tokenizer.pad_token_id)\n",
    "    new_batch[\"input_ids\"] = pad_to_max_length(new_batch[\"input_ids\"], max_length, pad_value=tokenizer.pad_token_id)\n",
    "\n",
    "    cached_batch[\"attention_mask\"] = pad_to_max_length(cached_batch[\"attention_mask\"], max_length, pad_value=0)\n",
    "    new_batch[\"attention_mask\"] = pad_to_max_length(new_batch[\"attention_mask\"], max_length, pad_value=0)\n",
    "\n",
    "    cached_batch[\"position_ids\"] = pad_to_max_length(cached_batch[\"position_ids\"], max_length, pad_value=0)\n",
    "    new_batch[\"position_ids\"] = pad_to_max_length(new_batch[\"position_ids\"], max_length, pad_value=0)\n",
    "\n",
    "    # Concatenate input_ids, attention_mask, position_ids, and past_key_values\n",
    "    merged_batch = {\n",
    "        \"input_ids\": torch.cat([cached_batch[\"input_ids\"], new_batch[\"input_ids\"]], dim=0),\n",
    "        \"attention_mask\": torch.cat([cached_batch[\"attention_mask\"], new_batch[\"attention_mask\"]], dim=0),\n",
    "        \"position_ids\": torch.cat([cached_batch[\"position_ids\"], new_batch[\"position_ids\"]], dim=0),\n",
    "        \"past_key_values\": None,\n",
    "        \"max_tokens\": torch.cat([cached_batch[\"max_tokens\"], new_batch[\"max_tokens\"]], dim=0)\n",
    "    }\n",
    "        # 合并 past_key_values\n",
    "    if cached_batch[\"past_key_values\"] is None:\n",
    "        # 如果 cached_batch 没有 past_key_values，直接使用 new_batch 的 past_key_values\n",
    "        merged_batch[\"past_key_values\"] = new_batch[\"past_key_values\"]\n",
    "    elif new_batch[\"past_key_values\"] is None:\n",
    "        # 如果 new_batch 没有 past_key_values，直接使用 cached_batch 的 past_key_values\n",
    "        merged_batch[\"past_key_values\"] = cached_batch[\"past_key_values\"]\n",
    "    else:\n",
    "        # 如果两者都有 past_key_values，需要正确合并\n",
    "        merged_past_key_values = []\n",
    "        for cached_layer, new_layer in zip(cached_batch[\"past_key_values\"], new_batch[\"past_key_values\"]):\n",
    "            # 合并 key 和 value\n",
    "            merged_key = torch.cat([cached_layer[0], new_layer[0]], dim=2)  # 在 batch_size 维度上合并\n",
    "            merged_value = torch.cat([cached_layer[1], new_layer[1]], dim=2)  # 在 batch_size 维度上合并\n",
    "            merged_past_key_values.append((merged_key, merged_value))\n",
    "        merged_batch[\"past_key_values\"] = tuple(merged_past_key_values)\n",
    "    return merged_batch\n",
    "\n",
    "\n",
    "def filter_batch(cached_batch):\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    finished_indices = []\n",
    "    for i in range(cached_batch[\"input_ids\"].size(0)):\n",
    "        # 检查是否生成 EOS token 或达到 max_tokens\n",
    "        if (\n",
    "                cached_batch[\"input_ids\"][i, -1] == eos_token_id or\n",
    "                cached_batch[\"attention_mask\"][i].sum() >= cached_batch[\"max_tokens\"][i]\n",
    "        ):\n",
    "            finished_indices.append(i)\n",
    "\n",
    "    # 移除已完成生成的请求\n",
    "    remaining_indices = [\n",
    "        i for i in range(cached_batch[\"input_ids\"].size(0))\n",
    "        if i not in finished_indices\n",
    "    ]\n",
    "    cached_batch[\"input_ids\"] = cached_batch[\"input_ids\"][remaining_indices]\n",
    "    cached_batch[\"position_ids\"] = cached_batch[\"position_ids\"][remaining_indices]\n",
    "    cached_batch[\"attention_mask\"] = cached_batch[\"attention_mask\"][remaining_indices]\n",
    "    cached_batch[\"max_tokens\"] = cached_batch[\"max_tokens\"][remaining_indices]\n",
    "    cached_batch[\"past_key_values\"] = None\n",
    "    # if cached_batch[\"past_key_values\"] is not None:\n",
    "    #     cached_batch[\"past_key_values\"] = None\n",
    "        # cached_batch[\"past_key_values\"] = [\n",
    "        #     [kv[remaining_indices] for kv in layer_kvs]\n",
    "        #     for layer_kvs in cached_batch[\"past_key_values\"]\n",
    "        # ]\n",
    "\n",
    "    return cached_batch, finished_indices\n"
   ],
   "id": "599385a1d84639ea",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:40:48.802189Z",
     "start_time": "2025-03-11T13:40:39.723173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_next_token(batch):\n",
    "    if \"past_key_values\" in batch:\n",
    "        sequence_length = batch[\"input_ids\"].shape[1]\n",
    "        print(sequence_length)\n",
    "        # 依次打印 past_key_values 的维度\n",
    "        # for layer_index, (key_tensor, value_tensor) in enumerate(batch[\"past_key_values\"]):\n",
    "        #     print(f\"Layer {layer_index + 1}:\")\n",
    "        #     print(f\"  Key tensor shape: {key_tensor.shape}\")\n",
    "        #     print(f\"  Value tensor shape: {value_tensor.shape}\")\n",
    "    # 依次打印 input_ids 的维度\n",
    "    inputs = batch\n",
    "    # create a list of tokens for every input in the batch\n",
    "    generated_tokens = [[] for _ in range(inputs[\"input_ids\"].shape[0])]\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "    next_inputs = {\n",
    "        \"position_ids\": position_ids,\n",
    "        **inputs\n",
    "    }\n",
    "\n",
    "    next_token_ids, past_key_values = generate_batch_tokens_with_past(next_inputs)\n",
    "    # 依次打印 past_key_values 的维度\n",
    "    # for layer_index, (key_tensor, value_tensor) in enumerate(past_key_values):\n",
    "    #     print(f\"Layer {layer_index + 1}:\")\n",
    "    #     print(f\"  Key tensor shape: {key_tensor.shape}\")\n",
    "    #     print(f\"  Value tensor shape: {value_tensor.shape}\")\n",
    "    next_inputs = {\n",
    "        \"input_ids\": torch.cat(\n",
    "            [next_inputs[\"input_ids\"], next_token_ids.reshape(-1, 1)],\n",
    "            dim=1\n",
    "        ),\n",
    "        \"position_ids\": torch.cat(\n",
    "            [next_inputs[\"position_ids\"],\n",
    "             next_inputs[\"position_ids\"][:, -1].unsqueeze(-1) + 1],\n",
    "            dim=1\n",
    "        ),\n",
    "        \"attention_mask\": torch.cat(\n",
    "            [next_inputs[\"attention_mask\"], torch.ones((next_token_ids.shape[0], 1))],\n",
    "            dim=1\n",
    "        ),\n",
    "        \"past_key_values\": None, # 我无法实现 past_key_values 的合并/拆分 相关逻辑，所以这里直接设置为 None。否则会报错\n",
    "        \"max_tokens\": batch[\"max_tokens\"]\n",
    "    }\n",
    "\n",
    "\n",
    "    next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "    for i, token in enumerate(next_tokens):\n",
    "        generated_tokens[i].append(token)\n",
    "    # print(\"\".join([\"\".join(tokens) for tokens in generated_tokens]))\n",
    "    return next_inputs\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "with tqdm(total=len(request_queue), desc=f\"bs={batch_size}\") as pbar:\n",
    "    # first, let's seed the initial cached_batch\n",
    "    # with the first `batch_size` inputs\n",
    "    # and run the initial prefill step\n",
    "    batch = init_batch(request_queue[:batch_size])\n",
    "    cached_batch = generate_next_token(batch)\n",
    "    request_queue = request_queue[batch_size:]\n",
    "\n",
    "    # continue until both the request queue is\n",
    "    # fully drained and every input\n",
    "    # within the cached_batch has completed generation\n",
    "    while (\n",
    "            len(request_queue) > 0 or\n",
    "            cached_batch[\"input_ids\"].size(0) > 0\n",
    "    ):\n",
    "        batch_capacity = (\n",
    "                batch_size - cached_batch['input_ids'].size(0)\n",
    "        )\n",
    "        if batch_capacity > 0 and len(request_queue) > 0:\n",
    "            # prefill\n",
    "            new_batch = init_batch(request_queue[:batch_capacity])\n",
    "            new_batch = generate_next_token(new_batch)\n",
    "            request_queue = request_queue[batch_capacity:]\n",
    "\n",
    "            # merge\n",
    "            cached_batch = merge_batches(cached_batch, new_batch)\n",
    "\n",
    "        # decode\n",
    "        cached_batch = generate_next_token(cached_batch)\n",
    "\n",
    "        # remove any inputs that have finished generation\n",
    "        cached_batch, removed_indices = filter_batch(cached_batch)\n",
    "        pbar.update(len(removed_indices))\n",
    "\n",
    "duration_s = time.time() - t0\n",
    "print(\"duration: \", duration_s)\n"
   ],
   "id": "4487bb1f45a2d6d5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bs=8:   6%|▋         | 2/32 [00:00<00:01, 18.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fence on be fence on be fence on\n",
      "8\n",
      " and the a and the a and the\n",
      "9\n",
      " ran first good ran first good ran first\n",
      " be fence\n",
      "10\n",
      " to day idea day idea day,,\n",
      "11\n",
      " the of. of. of then and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bs=8:  25%|██▌       | 8/32 [00:00<00:00, 35.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on be fence on\n",
      "12\n",
      " other\n",
      "\n",
      ",,,,,\n",
      " be fence\n",
      "13\n",
      " side is and then and and,,\n",
      " on\n",
      "14\n",
      " of the the, the then and,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bs=8:  38%|███▊      | 12/32 [00:00<00:00, 25.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " be\n",
      "15\n",
      " the fact rain is rain, and,\n",
      " fence on\n",
      "16\n",
      " fence that the is the then,,\n",
      " be\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bs=8:  56%|█████▋    | 18/32 [00:01<00:00, 21.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". the the rain, and and,\n",
      " fence on\n",
      "18\n",
      " He government in is the then,,\n",
      "19\n",
      " was is the the rain, and and\n",
      " be fence on\n",
      "20\n",
      " about not United is the,,,\n",
      "21\n",
      " to going States the rain then and and\n",
      " be fence\n",
      "22\n",
      " run to falls, the the,,\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bs=8:  69%|██████▉   | 22/32 [00:01<00:00, 16.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " when be on is fox rain then and\n",
      " on be\n",
      "24\n",
      " he able, the was,,,\n",
      " fence\n",
      "25\n",
      " saw to and about is and then,\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bs=8:  84%|████████▍ | 27/32 [00:01<00:00, 14.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the do the to the the, and\n",
      " on\n",
      "27\n",
      " fox anything rain run rain is,\n",
      "28\n",
      ". about in away the and\n",
      "29\n",
      " He it the when the\n",
      "30\n",
      " ran. United the rain\n",
      "31\n",
      " to\n",
      " States fox\n",
      "32\n",
      " the\n",
      " falls jumped\n",
      "33\n",
      " otherThe on over\n",
      "34\n",
      " side government, the\n",
      "35\n",
      " of is and fence\n",
      "36\n",
      " the going the.\n",
      "37\n",
      " fence to rain\n",
      "\n",
      "38\n",
      " and have in\n",
      "\n",
      "39\n",
      " ran to the\"\n",
      "40\n",
      " to do UnitedI\n",
      "41\n",
      " the something States'm\n",
      "42\n",
      " other about falls not\n",
      "43\n",
      " side it on going\n",
      "44\n",
      " of., to\n",
      "45\n",
      " the\n",
      " and let\n",
      "46\n",
      " fence\n",
      " the you\n",
      "47\n",
      ".The rain run\n",
      "48\n",
      " He government in away\n",
      "49\n",
      " ran is the,\"\n",
      "50\n",
      " to going United the\n",
      "51\n",
      " the to States fox\n",
      "52\n",
      " other have falls said\n",
      "53\n",
      " side to on.\n",
      "54\n",
      " of do,\n",
      "\n",
      "55\n",
      " the something and\n",
      "\n",
      "56\n",
      " fence about the\"\n",
      "57\n",
      " and it rainI\n",
      "58\n",
      " ran. in'm\n",
      "59\n",
      " to\n",
      " the not\n",
      "60\n",
      " the\n",
      " United going\n",
      "61\n",
      " otherThe States to\n",
      "62\n",
      " side government falls let\n",
      "63\n",
      " of is on you\n",
      "64\n",
      " the going, run\n",
      "65\n",
      " fence to and away\n",
      "66\n",
      ". have the,\"\n",
      "67\n",
      " He to rain the\n",
      "68\n",
      " ran do in fox\n",
      "69\n",
      " to something the said\n",
      "70\n",
      " the about United.\n",
      "71\n",
      " other it States\n",
      "\n",
      "72\n",
      " side. falls\n",
      "\n",
      "73\n",
      " of\n",
      " on\"\n",
      "74\n",
      " the\n",
      ",I\n",
      "75\n",
      " fenceThe and'm\n",
      "76\n",
      " and government the not\n",
      "77\n",
      " ran is rain going\n",
      "78\n",
      " to going in to\n",
      "79\n",
      " the to the let\n",
      "80\n",
      " other have United you\n",
      "81\n",
      " side to States run\n",
      "82\n",
      " of do falls away\n",
      "83\n",
      " the something on,\"\n",
      "84\n",
      " fence about, the\n",
      "85\n",
      ". it and fox\n",
      "86\n",
      " He. the said\n",
      "87\n",
      " ran\n",
      " rain.\n",
      "88\n",
      " to\n",
      " in\n",
      "\n",
      "89\n",
      " theThe the\n",
      "\n",
      "90\n",
      " other government United\"\n",
      "91\n",
      " side is StatesI\n",
      "92\n",
      " of going falls'm\n",
      "93\n",
      " the to on not\n",
      "94\n",
      " fence have, going\n",
      "95\n",
      " and to and to\n",
      "96\n",
      " ran do the let\n",
      "97\n",
      " to something rain you\n",
      "98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bs=8:  91%|█████████ | 29/32 [00:08<00:02,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the about in run\n",
      "99\n",
      " other it the away\n",
      "100\n",
      ". United,\"\n",
      "101\n",
      "\n",
      " States the\n",
      "102\n",
      "\n",
      " falls fox\n",
      "103\n",
      "The on said\n",
      "104\n",
      " government,.\n",
      "105\n",
      " and\n",
      "\n",
      "106\n",
      " the\n",
      "\n",
      "107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bs=8:  97%|█████████▋| 31/32 [00:08<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rain\"\n",
      "108\n",
      "I\n",
      "109\n",
      "'m\n",
      "110\n",
      " not\n",
      "111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bs=8: 100%|██████████| 32/32 [00:09<00:00,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " going\n",
      "duration:  9.074449062347412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:40:48.806935Z",
     "start_time": "2025-03-11T13:40:48.805565Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9c22677601915814",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:37:27.027898Z",
     "start_time": "2025-03-11T13:37:27.026595Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a7154db23712b665",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:26:55.206782Z",
     "start_time": "2025-03-11T13:26:55.205522Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "56a6219361c81bb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:26:55.215486Z",
     "start_time": "2025-03-11T13:26:55.214223Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "871d579ceba0add9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
